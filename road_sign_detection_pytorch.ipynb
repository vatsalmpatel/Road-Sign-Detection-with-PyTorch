{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading dataset from kaggle notebooks and extracting the images and labels"
      ],
      "metadata": {
        "id": "bBkpUzBrAzln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kaggle\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "8Bf3rOwz684Q",
        "outputId": "94d86dc6-d12f-4829-90a1-12a64ea04229"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-865be0e3-b836-4bef-9c1d-423c0c3a1364\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-865be0e3-b836-4bef-9c1d-423c0c3a1364\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d andrewmvd/road-sign-detection"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aoiA3oP6804",
        "outputId": "2f825507-5fef-44f8-d18f-dc9752a4b1dc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading road-sign-detection.zip to /content\n",
            " 99% 216M/218M [00:11<00:00, 18.0MB/s]\n",
            "100% 218M/218M [00:11<00:00, 20.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q road-sign-detection.zip"
      ],
      "metadata": {
        "id": "cHQklZVl68x4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/annotations /content/labels"
      ],
      "metadata": {
        "id": "a3iDOTzH68vS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a function to convert VOC annotations (which are in XML) to YOLOv5 annotations (which need to be in a txt format) with its correct annotated class within the txt file. So this function will do it automatically and store it insode the labels folder"
      ],
      "metadata": {
        "id": "XAKg5rqBA_J2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "def convert_box(size, box):\n",
        "  dw, dh = 1. / size[0], 1. / size[1]\n",
        "  x, y, w, h = (box[0] + box[1]) / 2.0 - 1, (box[2] + box[3]) / 2.0 - 1, box[1] - box[0], box[3] - box[2]\n",
        "  return x * dw, y * dh, w * dw, h * dh\n",
        "\n",
        "def convert_voc_to_yolo():\n",
        "  for anno in os.listdir('/content/labels'):\n",
        "    if anno.split('.')[1] == 'xml':\n",
        "      file_name = anno.split('.')[0]\n",
        "      out_file = open(f'/content/labels/{file_name}.txt', 'w')\n",
        "\n",
        "      tree = ET.parse('/content/labels/' + anno)\n",
        "      root = tree.getroot()\n",
        "      size = root.find('size')        \n",
        "      w = int(size.find('width').text)\n",
        "      h = int(size.find('height').text)\n",
        "\n",
        "      names = ['trafficlight', 'speedlimit', 'crosswalk', 'stop']\n",
        "\n",
        "      for obj in root.iter('object'):\n",
        "        cls = obj.find('name').text\n",
        "        if cls in names and int(obj.find('difficult').text) != 1:\n",
        "          xmlbox = obj.find('bndbox')\n",
        "          bb = convert_box((w, h), [float(xmlbox.find(x).text) for x in ('xmin', 'xmax', 'ymin', 'ymax')])\n",
        "          cls_id = names.index(cls)\n",
        "          out_file.write(\" \".join([str(a) for a in (cls_id, *bb)]) + '\\n')\n",
        "  print(\"Conversion from VOC format to YOLO format Completed\")"
      ],
      "metadata": {
        "id": "Rv87Ko0368sa"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "convert_voc_to_yolo()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7CdfSDB68p4",
        "outputId": "fb0ed3b8-e6a4-42e3-9c87-e8b72b92f80a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conversion from VOC format to YOLO format Completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cloning the YOLOv5 GitHub repo to use their training scripts"
      ],
      "metadata": {
        "id": "UcK-3GqkBYbP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usaLI5zQ59-N",
        "outputId": "b9a9af9c-f6a9-45db-9b4a-56129eb47aa1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 14302, done.\u001b[K\n",
            "remote: Counting objects: 100% (56/56), done.\u001b[K\n",
            "remote: Compressing objects: 100% (41/41), done.\u001b[K\n",
            "remote: Total 14302 (delta 26), reused 30 (delta 15), pack-reused 14246\u001b[K\n",
            "Receiving objects: 100% (14302/14302), 13.62 MiB | 17.28 MiB/s, done.\n",
            "Resolving deltas: 100% (9836/9836), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ultralytics/yolov5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -r /content/yolov5/requirements.txt"
      ],
      "metadata": {
        "id": "bdiFQNi96Z8Z"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## After you run this below command, you need to change the yaml file to accomodate your new classes durig training and specify correct paths to find the annotations and images for training the model. Look at the README file to see what the VOC.yaml file should look like"
      ],
      "metadata": {
        "id": "LfLoZwWxBj4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/yolov5/data/VOC.yaml /content/yolov5/VOC.yaml"
      ],
      "metadata": {
        "id": "HLlvOFCA86aS"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running the training script by specifying the training YAML file and the weights file"
      ],
      "metadata": {
        "id": "JyjXa_8wB60K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/yolov5/train.py --img 320 --batch 16 --epochs 50 \\\n",
        "--data /content/yolov5/VOC.yaml --weights /content/yolov5/yolov5s.pt --workers 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_9EC1d09oCm",
        "outputId": "3fc6376c-9e28-4142-88b3-81d771623014"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=/content/yolov5/yolov5s.pt, cfg=, data=/content/yolov5/VOC.yaml, hyp=yolov5/data/hyps/hyp.scratch-low.yaml, epochs=50, batch_size=16, imgsz=320, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=2, project=yolov5/runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\n",
            "YOLOv5 🚀 v7.0-18-gd7955fe Python-3.8.15 torch-1.12.1+cu113 CUDA:0 (Tesla T4, 15110MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n",
            "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir yolov5/runs/train', view at http://localhost:6006/\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n",
            "100% 755k/755k [00:00<00:00, 3.41MB/s]\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to /content/yolov5/yolov5s.pt...\n",
            "100% 14.1M/14.1M [00:03<00:00, 3.77MB/s]\n",
            "\n",
            "Overriding model.yaml nc=80 with nc=4\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1     24273  models.yolo.Detect                      [4, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "Model summary: 214 layers, 7030417 parameters, 7030417 gradients, 16.0 GFLOPs\n",
            "\n",
            "Transferred 343/349 items from /content/yolov5/yolov5s.pt\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/labels... 877 images, 0 backgrounds, 0 corrupt: 100% 877/877 [00:00<00:00, 1027.56it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/labels.cache\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/labels.cache... 877 images, 0 backgrounds, 0 corrupt: 100% 877/877 [00:00<?, ?it/s]\n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m5.44 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset ✅\n",
            "Plotting labels to yolov5/runs/train/exp/labels.jpg... \n",
            "Image sizes 320 train, 320 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1myolov5/runs/train/exp\u001b[0m\n",
            "Starting training for 50 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       0/49     0.963G     0.1045    0.01822    0.03643         24        320: 100% 55/55 [00:28<00:00,  1.95it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 28/28 [00:11<00:00,  2.53it/s]\n",
            "                   all        877       1244      0.545      0.115     0.0993     0.0344\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       1/49      1.24G    0.07527    0.02081    0.02203         38        320: 100% 55/55 [00:25<00:00,  2.18it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 28/28 [00:10<00:00,  2.72it/s]\n",
            "                   all        877       1244      0.877      0.167      0.249     0.0813\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       2/49      1.24G     0.0677    0.01675    0.01563         30        320: 100% 55/55 [00:25<00:00,  2.16it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 28/28 [00:12<00:00,  2.17it/s]\n",
            "                   all        877       1244      0.347      0.548      0.373      0.142\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       3/49      1.24G    0.06084    0.01516    0.01105         41        320: 100% 55/55 [00:25<00:00,  2.18it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 28/28 [00:09<00:00,  2.81it/s]\n",
            "                   all        877       1244      0.489      0.552      0.478      0.221\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       4/49      1.24G    0.05273    0.01307   0.007715         38        320: 100% 55/55 [00:24<00:00,  2.22it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 28/28 [00:10<00:00,  2.73it/s]\n",
            "                   all        877       1244       0.72      0.708      0.737      0.317\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       5/49      1.24G    0.04768    0.01152    0.00641         43        320: 100% 55/55 [00:24<00:00,  2.23it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 28/28 [00:09<00:00,  2.80it/s]\n",
            "                   all        877       1244      0.579      0.709      0.609       0.31\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       6/49      1.24G    0.04399    0.01084   0.005424         37        320: 100% 55/55 [00:32<00:00,  1.70it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 28/28 [00:11<00:00,  2.46it/s]\n",
            "                   all        877       1244      0.799      0.808      0.845      0.446\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       7/49      1.24G    0.04182    0.01045    0.00486         34        320: 100% 55/55 [00:25<00:00,  2.18it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 28/28 [00:11<00:00,  2.45it/s]\n",
            "                   all        877       1244      0.818      0.835      0.847      0.451\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       8/49      1.24G     0.0388   0.009727   0.004345         28        320: 100% 55/55 [00:24<00:00,  2.21it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 28/28 [00:10<00:00,  2.79it/s]\n",
            "                   all        877       1244      0.878      0.827      0.876      0.515\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       9/49      1.24G    0.03692   0.009369     0.0042         32        320: 100% 55/55 [00:36<00:00,  1.50it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 28/28 [00:11<00:00,  2.44it/s]\n",
            "                   all        877       1244      0.932      0.848      0.912      0.521\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      10/49      1.24G    0.03582   0.009186   0.003758         43        320: 100% 55/55 [00:28<00:00,  1.93it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 28/28 [00:10<00:00,  2.73it/s]\n",
            "                   all        877       1244      0.897      0.867        0.9      0.503\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      11/49      1.24G    0.03544   0.009378   0.003715         35        320: 100% 55/55 [00:28<00:00,  1.95it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 28/28 [00:11<00:00,  2.36it/s]\n",
            "                   all        877       1244      0.955      0.861      0.922      0.547\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      12/49      1.24G    0.03396   0.008769    0.00366         26        320: 100% 55/55 [00:28<00:00,  1.93it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 28/28 [00:11<00:00,  2.46it/s]\n",
            "                   all        877       1244      0.938      0.869      0.932      0.603\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      13/49      1.24G    0.03226   0.008663   0.002965         34        320: 100% 55/55 [00:25<00:00,  2.16it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 28/28 [00:11<00:00,  2.51it/s]\n",
            "                   all        877       1244      0.951      0.878      0.931      0.548\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      14/49      1.24G    0.03154   0.008288   0.003241         42        320: 100% 55/55 [00:27<00:00,  1.99it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 28/28 [00:13<00:00,  2.11it/s]\n",
            "                   all        877       1244      0.914      0.903       0.92      0.609\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      15/49      1.24G     0.0306   0.008194    0.00291         35        320: 100% 55/55 [00:30<00:00,  1.78it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 28/28 [00:10<00:00,  2.55it/s]\n",
            "                   all        877       1244      0.954      0.877      0.936      0.618\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      16/49      1.24G    0.03013   0.008395   0.002589         28        320: 100% 55/55 [00:27<00:00,  1.98it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 28/28 [00:10<00:00,  2.56it/s]\n",
            "                   all        877       1244      0.926      0.887      0.939      0.629\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      17/49      1.24G    0.02933   0.007848   0.002852         28        320: 100% 55/55 [00:25<00:00,  2.15it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 28/28 [00:11<00:00,  2.44it/s]\n",
            "                   all        877       1244      0.962      0.904      0.947      0.636\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      18/49      1.24G    0.02881   0.007759   0.002458         42        320: 100% 55/55 [00:25<00:00,  2.12it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 28/28 [00:09<00:00,  2.86it/s]\n",
            "                   all        877       1244      0.951      0.909      0.938      0.642\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      19/49      1.24G    0.02788   0.007765   0.002308         19        320: 100% 55/55 [00:24<00:00,  2.21it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 28/28 [00:09<00:00,  3.03it/s]\n",
            "                   all        877       1244      0.956      0.916      0.952      0.605\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      20/49      1.24G    0.02655   0.007375   0.002398         39        320: 100% 55/55 [00:24<00:00,  2.22it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 28/28 [00:13<00:00,  2.15it/s]\n",
            "                   all        877       1244       0.96      0.921      0.954      0.671\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      21/49      1.24G    0.02643   0.007504   0.002322         31        320: 100% 55/55 [00:25<00:00,  2.20it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 28/28 [00:09<00:00,  2.92it/s]\n",
            "                   all        877       1244      0.971      0.913      0.951      0.672\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      22/49      1.24G     0.0263   0.007277   0.002125         31        320: 100% 55/55 [00:25<00:00,  2.19it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 28/28 [00:09<00:00,  2.88it/s]\n",
            "                   all        877       1244      0.958      0.914      0.953      0.674\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      23/49      1.24G    0.02657   0.007818    0.00205         30        320: 100% 55/55 [00:24<00:00,  2.20it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 28/28 [00:09<00:00,  2.85it/s]\n",
            "                   all        877       1244      0.971      0.918       0.96      0.692\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      24/49      1.24G    0.02566   0.007349    0.00212         30        320: 100% 55/55 [00:24<00:00,  2.21it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 28/28 [00:10<00:00,  2.77it/s]\n",
            "                   all        877       1244      0.961      0.916      0.962      0.714\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      25/49      1.24G    0.02495   0.007312   0.002115         40        320: 100% 55/55 [00:24<00:00,  2.24it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 28/28 [00:10<00:00,  2.80it/s]\n",
            "                   all        877       1244      0.965       0.92      0.964      0.703\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      26/49      1.24G    0.02372   0.007051   0.001902         29        320: 100% 55/55 [00:25<00:00,  2.18it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 28/28 [00:09<00:00,  2.88it/s]\n",
            "                   all        877       1244      0.946       0.93      0.967      0.734\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      27/49      1.24G     0.0225   0.007127   0.001755         32        320: 100% 55/55 [00:28<00:00,  1.94it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 28/28 [00:11<00:00,  2.53it/s]\n",
            "                   all        877       1244      0.957      0.936      0.963      0.708\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      28/49      1.24G    0.02297   0.007106   0.001807         30        320: 100% 55/55 [00:28<00:00,  1.94it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 28/28 [00:09<00:00,  2.86it/s]\n",
            "                   all        877       1244       0.97      0.927      0.968       0.72\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      29/49      1.24G    0.02274   0.006893    0.00167         27        320: 100% 55/55 [00:25<00:00,  2.17it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 28/28 [00:10<00:00,  2.79it/s]\n",
            "                   all        877       1244       0.95      0.935      0.965      0.705\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      30/49      1.24G    0.02231   0.006835   0.001788         30        320: 100% 55/55 [00:24<00:00,  2.22it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 28/28 [00:10<00:00,  2.76it/s]\n",
            "                   all        877       1244      0.966      0.943      0.969      0.747\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      31/49      1.24G    0.02174   0.006953   0.001767         47        320: 100% 55/55 [00:25<00:00,  2.18it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 28/28 [00:09<00:00,  2.88it/s]\n",
            "                   all        877       1244      0.965      0.945      0.967      0.708\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      32/49      1.24G    0.02123   0.006882   0.001764         29        320: 100% 55/55 [00:24<00:00,  2.21it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 28/28 [00:09<00:00,  2.84it/s]\n",
            "                   all        877       1244       0.97      0.944       0.97      0.726\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      33/49      1.24G    0.02045   0.006544   0.001445         40        320: 100% 55/55 [00:27<00:00,  1.97it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 28/28 [00:09<00:00,  2.82it/s]\n",
            "                   all        877       1244      0.967      0.943      0.976      0.756\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      34/49      1.24G     0.0202   0.006667   0.001666         35        320: 100% 55/55 [00:25<00:00,  2.19it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 28/28 [00:09<00:00,  2.82it/s]\n",
            "                   all        877       1244       0.98      0.939      0.975      0.755\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      35/49      1.24G    0.02009   0.006474   0.001373         30        320: 100% 55/55 [00:24<00:00,  2.24it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 28/28 [00:09<00:00,  2.88it/s]\n",
            "                   all        877       1244      0.969      0.935      0.971      0.742\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      36/49      1.24G    0.01899   0.006444   0.001357         36        320: 100% 55/55 [00:25<00:00,  2.16it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 28/28 [00:10<00:00,  2.78it/s]\n",
            "                   all        877       1244      0.968      0.939      0.973      0.744\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      37/49      1.24G    0.01954   0.006201   0.001421         45        320: 100% 55/55 [00:24<00:00,  2.24it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 28/28 [00:10<00:00,  2.78it/s]\n",
            "                   all        877       1244      0.969      0.945      0.975      0.766\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      38/49      1.24G    0.01851    0.00626   0.001363         35        320: 100% 55/55 [00:25<00:00,  2.15it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 28/28 [00:09<00:00,  2.99it/s]\n",
            "                   all        877       1244      0.967       0.94       0.97      0.766\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      39/49      1.24G    0.01826   0.006367   0.001194         39        320: 100% 55/55 [00:27<00:00,  2.00it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 28/28 [00:10<00:00,  2.61it/s]\n",
            "                   all        877       1244      0.976      0.935      0.971      0.763\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      40/49      1.24G    0.01775   0.006158     0.0012         34        320: 100% 55/55 [00:25<00:00,  2.18it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 28/28 [00:09<00:00,  2.91it/s]\n",
            "                   all        877       1244       0.97       0.95      0.976      0.778\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      41/49      1.24G     0.0178   0.006226   0.001113         28        320: 100% 55/55 [00:24<00:00,  2.22it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 28/28 [00:10<00:00,  2.72it/s]\n",
            "                   all        877       1244      0.973      0.947      0.976      0.786\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      42/49      1.24G     0.0171   0.005812   0.001059         30        320: 100% 55/55 [00:24<00:00,  2.22it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 28/28 [00:09<00:00,  2.81it/s]\n",
            "                   all        877       1244      0.973      0.932      0.976      0.791\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      43/49      1.24G    0.01714   0.006159  0.0009784         27        320: 100% 55/55 [00:25<00:00,  2.18it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 28/28 [00:09<00:00,  2.95it/s]\n",
            "                   all        877       1244      0.973      0.944      0.976      0.779\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      44/49      1.24G    0.01687   0.006053   0.001008         31        320: 100% 55/55 [00:25<00:00,  2.20it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 28/28 [00:09<00:00,  2.82it/s]\n",
            "                   all        877       1244      0.976      0.939      0.977      0.789\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      45/49      1.24G    0.01606   0.006061    0.00088         28        320: 100% 55/55 [00:25<00:00,  2.15it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 28/28 [00:11<00:00,  2.49it/s]\n",
            "                   all        877       1244      0.983      0.934      0.977        0.8\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      46/49      1.24G    0.01588   0.005928  0.0009649         45        320: 100% 55/55 [00:25<00:00,  2.15it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 28/28 [00:10<00:00,  2.76it/s]\n",
            "                   all        877       1244      0.967       0.95      0.979      0.797\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      47/49      1.24G    0.01563   0.005812   0.000857         35        320: 100% 55/55 [00:25<00:00,  2.18it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 28/28 [00:09<00:00,  2.89it/s]\n",
            "                   all        877       1244      0.966      0.948      0.978      0.802\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      48/49      1.24G    0.01502   0.005617  0.0009117         29        320: 100% 55/55 [00:24<00:00,  2.23it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 28/28 [00:10<00:00,  2.75it/s]\n",
            "                   all        877       1244      0.968      0.936      0.972      0.799\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      49/49      1.24G     0.0146   0.005516    0.00073         26        320: 100% 55/55 [00:25<00:00,  2.17it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 28/28 [00:10<00:00,  2.80it/s]\n",
            "                   all        877       1244      0.969      0.944      0.975      0.807\n",
            "\n",
            "50 epochs completed in 0.523 hours.\n",
            "Optimizer stripped from yolov5/runs/train/exp/weights/last.pt, 14.3MB\n",
            "Optimizer stripped from yolov5/runs/train/exp/weights/best.pt, 14.3MB\n",
            "\n",
            "Validating yolov5/runs/train/exp/weights/best.pt...\n",
            "Fusing layers... \n",
            "Model summary: 157 layers, 7020913 parameters, 0 gradients, 15.8 GFLOPs\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 28/28 [00:10<00:00,  2.77it/s]\n",
            "                   all        877       1244      0.969      0.944      0.975      0.806\n",
            "          trafficlight        877        170      0.942      0.882      0.948      0.696\n",
            "            speedlimit        877        783      0.995      0.995      0.994      0.888\n",
            "             crosswalk        877        200      0.958       0.91      0.963      0.751\n",
            "                  stop        877         91       0.98      0.989      0.994      0.891\n",
            "Results saved to \u001b[1myolov5/runs/train/exp\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using the model we just trained to generate inferences"
      ],
      "metadata": {
        "id": "jGbTvZlbDe-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import itertools as it\n",
        "import os"
      ],
      "metadata": {
        "id": "zExa-u34-1Cb"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## use this to detect signs in Images"
      ],
      "metadata": {
        "id": "Kxlg-YBcJJwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/yolov5/detect.py --weights /content/yolov5/runs/train/exp/weights/best.pt --img 320 --conf 0.1 --source /content/images/road95.png"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6yoyf09G4N9",
        "outputId": "bfc429c9-32e7-4144-da5d-090dd84f8a0c"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['/content/yolov5/runs/train/exp/weights/best.pt'], source=/content/images/road95.png, data=yolov5/data/coco128.yaml, imgsz=[320, 320], conf_thres=0.1, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=yolov5/runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
            "YOLOv5 🚀 v7.0-18-gd7955fe Python-3.8.15 torch-1.12.1+cu113 CUDA:0 (Tesla T4, 15110MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 157 layers, 7020913 parameters, 0 gradients, 15.8 GFLOPs\n",
            "image 1/1 /content/images/road95.png: 320x256 1 stop, 11.0ms\n",
            "Speed: 0.2ms pre-process, 11.0ms inference, 1.3ms NMS per image at shape (1, 3, 320, 320)\n",
            "Results saved to \u001b[1myolov5/runs/detect/exp7\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WZSU4Rv-IQLt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}